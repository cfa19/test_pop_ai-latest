# Supabase Database Schema - RAG General

**Version:** 2.0
**Last Updated:** January 2026
**Database:** PostgreSQL 15+ with pgvector extension

---

## Overview

This schema implements **hybrid search** combining three approaches:

1. **Semantic search** - Uses HNSW vector index (pgvector) for embedding similarity
2. **Full-text search** - Uses GIN index (PostgreSQL tsvector) for keyword matching
3. **RRF fusion** - Reciprocal Rank Fusion to combine both ranking methods

---

## Table: `general_embeddings_1024`

Stores document chunks with embeddings for hybrid search.

| Column | Type | Description |
|--------|------|-------------|
| `id` | BIGSERIAL PRIMARY KEY | Auto-increment unique identifier |
| `content` | TEXT NOT NULL | The chunk text content |
| `embedding` | VECTOR(1024) | Embedding vector from Voyage AI (`voyage-3-large`) |
| `content_tsvector` | TSVECTOR | Full-text search vector (auto-generated by trigger) |
| `metadata` | JSONB | Chunk metadata (source, index, model) |
| `created_at` | TIMESTAMPTZ | Creation timestamp |

### Metadata Structure

Each chunk stores metadata with the following fields:
- **source**: Original filename (e.g., "general_info_chunks.md")
- **chunk_index**: Position in the document (0-based)
- **chunk_id**: Chunk ID from the source file
- **total_chunks**: Total number of chunks from the source
- **dimensions**: Embedding dimensions (1024)
- **model**: Model used (`voyage-3-large`)
- **section**: Section name (optional)
- **subsection**: Subsection name (optional)
- **title**: Chunk title (optional)

---

## Indexes

### HNSW Index (Semantic Search)

The **HNSW** (Hierarchical Navigable Small World) index enables fast approximate nearest neighbor search with O(log n) complexity.

**Index name:** `general_embeddings_1024_embedding_idx`

| Parameter | Value | Description |
|-----------|-------|-------------|
| m | 16 | Number of bi-directional links per node (higher = better quality, more memory) |
| ef_construction | 64 | Size of dynamic candidate list during index construction |
| vector_cosine_ops | - | Uses cosine distance for similarity calculation |

### GIN Index (Full-Text Search)

The **GIN** (Generalized Inverted Index) enables fast keyword-based full-text search using PostgreSQL's tsvector type.

**Index name:** `general_embeddings_1024_tsvector_idx`

---

## Trigger: Auto-generate tsvector

A trigger function `update_content_tsvector()` automatically generates Spanish language tsvector on every insert/update. The trigger `tsvector_update` fires BEFORE INSERT OR UPDATE on the table.

To change the language, modify `'spanish'` to `'english'` in the trigger function.

---

## RPC Functions

### `rag_search_semantic`

Pure semantic search using embedding cosine similarity.

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| query_embedding | VECTOR(1024) | - | Query embedding vector |
| match_count | INT | 5 | Number of results to return |

**Returns:** id, content, similarity (0-1 score, higher is better)

**How it works:** Calculates cosine similarity between the query embedding and all document embeddings, returning the top matches ordered by similarity score.

**Type casting:** The similarity score is cast to FLOAT to ensure compatibility with the function return type.

---

### `rag_search_fulltext`

Pure keyword-based full-text search using PostgreSQL's tsvector.

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| query_text | TEXT | - | Search query in natural language |
| match_count | INT | 5 | Number of results to return |

**Returns:** id, content, rank (higher is better)

**How it works:** Converts the query to a tsquery, matches against the content_tsvector column, and ranks results using PostgreSQL's ts_rank function.

**Type casting:** The rank score is cast to FLOAT to ensure compatibility with the function return type.

---

### `rag_hybrid_search_user_context`

Hybrid search combining semantic + full-text using RRF (Reciprocal Rank Fusion).

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| query_text | TEXT | - | Search query text for full-text |
| query_embedding | VECTOR(1024) | - | Query embedding vector for semantic |
| match_count | INT | 5 | Number of results to return |
| semantic_weight | FLOAT | 0.5 | Balance: 1.0=semantic only, 0.0=fulltext only |
| rrf_k | INT | 60 | RRF smoothing constant |

**Returns:** id, content, semantic_rank, fulltext_rank, rrf_score

**How it works:**

1. **Semantic CTE**: Retrieves top (match_count * 2) results ordered by embedding similarity
2. **Full-text CTE**: Retrieves top (match_count * 2) results matching the text query
3. **Combined CTE**: Performs FULL OUTER JOIN and calculates RRF score
4. **Final SELECT**: Returns top match_count results ordered by RRF score

**RRF Formula:**

```
rrf_score = semantic_weight * (1 / (k + semantic_rank)) + (1 - semantic_weight) * (1 / (k + fulltext_rank))
```

**Type casting:** The rrf_score is cast to FLOAT and rank values are cast to INT to ensure compatibility with the function return types. This fix resolves the "real does not match expected type double precision" error.

---

## Embedding Configuration

### Why Voyage AI `voyage-3-large`?

| Aspect | Value |
|--------|-------|
| Provider | Voyage AI |
| Model | `voyage-3-large` |
| Native dimensions | 1024 |
| HNSW limit | 2000 (pgvector) |
| Compatible | Yes (1024 < 2000) |

Voyage AI's `voyage-3-large` produces 1024-dimensional embeddings natively, well within pgvector's HNSW index limit of 2000 dimensions. No dimensionality reduction is needed.

### Storage Characteristics

| Aspect | Value |
|--------|-------|
| Storage per vector | 4,096 bytes |
| Vector index type | HNSW (cosine) |
| Search complexity | O(log n) |

### Configuration Files

All project files use `EMBED_DIMENSIONS = 1024`:

| File | Configuration |
|------|---------------|
| `rag/load_embeddings.py` | `EMBED_DIMENSIONS = 1024` |
| `rag/test_rag.py` | `EMBED_DIMENSIONS = 1024` |
| `rag/benchmarks.py` | `EMBED_DIMENSIONS = 1024` |
| `sql/01_setup_supabase.sql` | `VECTOR(1024)` |
| `src/utils/rag.py` | Shared search & embedding functions |
| `.env` | `EMBED_MODEL=voyage-3-large`, `EMBED_DIMENSIONS=1024` |

---

## Type Casting Fix (v1.2)

**Problem:** PostgreSQL's `ts_rank()` function returns type `real`, but the function declarations specify `FLOAT` (which is `double precision`). This caused error 42804: "Returned type real does not match expected type double precision".

**Solution:** Added explicit `::FLOAT` casts to all score calculations:

- `rag_search_semantic`: similarity score cast to FLOAT
- `rag_search_fulltext`: rank score cast to FLOAT
- `rag_hybrid_search_user_context`: rrf_score cast to FLOAT

---

## Required Extension

The pgvector extension must be enabled in Supabase before creating the table and indexes.

---

## Verification

To verify the schema is working correctly:

1. Check document count and embedding dimensions in the table
2. Expected avg_dimensions = 1024
3. Test each RPC function individually before running benchmarks
